from __future__ import annotations
import os
import math
from typing import List, Tuple
import google.generativeai as genai
from app.utils.schema import Chunk
from app.utils.logger import rag_logger

LLM_MODEL = os.getenv("RAG_LLM_MODEL", "gemini-1.5-flash")
TEMP = float(os.getenv("GEN_TEMPERATURE", "0.1"))
MAX_OUT = int(
    os.getenv("GEN_MAX_OUTPUT_TOKENS", "768")
)  # CÃ¢n báº±ng: 768 tokens Ä‘á»§ chi tiáº¿t nhÆ°ng tiáº¿t kiá»‡m hÆ¡n 1024

SYSTEM_PROMPT = (
    "Báº¡n lÃ  trá»£ lÃ½ RAG, tráº£ lá»i báº±ng tiáº¿ng Viá»‡t dá»±a 100% trÃªn Ná»˜I DUNG TÃ€I LIá»†U. "
    "LuÃ´n so sÃ¡nh cÃ¢u há»i vá»›i ngá»¯ cáº£nh vÃ  chá»‰ sá»­ dá»¥ng thÃ´ng tin liÃªn quan.\n"
    "ğŸ“‹ Bá» Cá»¤C TRáº¢ Lá»œI:\n"
    "1. ğŸ¯ Káº¿t luáº­n: 1â€“3 cÃ¢u tráº£ lá»i trá»±c tiáº¿p, cÃ³ [tÃªn_file.pdf:trang].\n"
    "2. ğŸ“š Dáº«n chá»©ng: 1â€“3 bullet dáº«n chá»©ng quan trá»ng, má»—i bullet kÃ¨m [tÃªn_file.pdf:trang].\n"
    # "3. ğŸ“ PhÃ¢n tÃ­ch: Giáº£i thÃ­ch ngáº¯n gá»n hÆ¡n náº¿u cáº§n, chá»‰ thÃ´ng tin sÃ¡t cÃ¢u há»i.\n"
    "3. âš ï¸ LÆ°u Ã½/Khuyáº¿n nghá»‹: NÃªu háº¡n cháº¿ hoáº·c gá»£i Ã½ hÃ nh Ä‘á»™ng tiáº¿p theo (náº¿u cÃ³).\n"
    "ğŸ“Œ QUY Táº®C:\n"
    "â€¢ KhÃ´ng bá»‹a Ä‘áº·t; náº¿u thiáº¿u dá»¯ liá»‡u, tráº£ lá»i: 'Xin lá»—i, khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p trong tÃ i liá»‡u hiá»‡n cÃ³.'\n"
    "â€¢ Chá»‰ dÃ¹ng thÃ´ng tin tá»« tÃ i liá»‡u; khÃ´ng thÃªm kiáº¿n thá»©c ngoÃ i.\n"
    "â€¢ Khi trÃ­ch dáº«n, ghi Ä‘Ãºng tÃªn file vÃ  sá»‘ trang, vÃ­ dá»¥ [tb741.pdf:3].\n"
    "â€¢ KhÃ´ng láº·p láº¡i cÃ¢u há»i, khÃ´ng xin lá»—i nhiá»u láº§n.\n"
    "â€¢ TrÃ¬nh bÃ y báº±ng markdown rÃµ rÃ ng, ngáº¯n gá»n, chuyÃªn nghiá»‡p vÃ  thÃ¢n thiá»‡n."
)


def _sigmoid(value: float | None) -> float:
    if value is None:
        return 0.0
    try:
        v = float(value)
    except (TypeError, ValueError):
        return 0.0
    if v >= 50:
        return 1.0
    if v <= -50:
        return 0.0
    return 1.0 / (1.0 + math.exp(-v))


def _ensure_init():
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("GEMINI_API_KEY chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh.")
    genai.configure(api_key=api_key)


def _build_context(passages: List[Chunk]) -> str:
    parts = []
    for idx, p in enumerate(passages, start=1):
        relevance = 0.0
        if isinstance(getattr(p, "meta", None), dict):
            relevance = float(p.meta.get("relevance", 0.0) or 0.0)
        elif getattr(p, "score", None) is not None:
            relevance = _sigmoid(p.score)
        header = f"[{idx}] {p.doc_name}:{p.page} (Ä‘á»™ phÃ¹ há»£p â‰ˆ {relevance:.2f})"
        text = p.text.replace("\n", " ").strip()
        if len(text) > 1200:
            text = text[:1200] + "â€¦"
        parts.append(f"{header}\n{text}")
    return "\n\n".join(parts)


def generate(query: str, passages: List[Chunk]) -> Tuple[str, float]:
    _ensure_init()
    context = _build_context(passages)

    # Prompt tá»‘i Æ°u cho format Ä‘áº¹p vÃ  cÃ¢u tráº£ lá»i Ä‘áº§y Ä‘á»§
    prompt = f"""{SYSTEM_PROMPT}

=== THÃ”NG TIN Tá»ª TÃ€I LIá»†U ===
{context}

=== CÃ‚U Há»I ===
{query}

=== YÃŠU Cáº¦U ===
HÃ£y tráº£ lá»i THEO ÄÃšNG Cáº¤U TRÃšC trÃªn vá»›i:
- Xuá»‘ng dÃ²ng rÃµ rÃ ng giá»¯a cÃ¡c pháº§n
- Sá»­ dá»¥ng **bold** cho tiÃªu Ä‘á» 
- Bullet points cho danh sÃ¡ch
- Tráº£ lá»i Äáº¦Y Äá»¦, khÃ´ng Ä‘Æ°á»£c cáº¯t giá»¯a chá»«ng
- TRÃCH DáºªN: Sá»­ dá»¥ng tÃªn file CHÃNH XÃC tá»« context á»Ÿ trÃªn, vÃ­ dá»¥ [tb741.pdf:2] chá»© KHÃ”NG PHáº¢I [doc:1]"""

    model = genai.GenerativeModel(
        LLM_MODEL,
        generation_config={
            "temperature": TEMP,
            "max_output_tokens": MAX_OUT,
            "candidate_count": 1,
        },
    )
    resp = model.generate_content(prompt)

    feedback = getattr(resp, "prompt_feedback", None)
    if feedback and getattr(feedback, "block_reason", None):
        raise RuntimeError(f"YÃªu cáº§u bá»‹ tá»« chá»‘i: {feedback.block_reason}")

    answer = ""
    resp_text_error: str | None = None
    # resp.text accessor may raise ValueError if the response doesn't contain Part objects
    try:
        if hasattr(resp, "text") and resp.text:
            answer = resp.text.strip()
    except ValueError as e:
        # keep the error message for debugging / fallback behavior
        resp_text_error = str(e)

    # Fallback: try to extract from candidates (older or alternate response shape)
    if not answer and getattr(resp, "candidates", None):
        for candidate in resp.candidates:
            parts = getattr(getattr(candidate, "content", None), "parts", [])
            texts = [p.text for p in parts if getattr(p, "text", None)]
            if texts:
                answer = "\n".join(texts).strip()
                if answer:
                    break

    # If still empty, handle gracefully (do not raise unhandled exception)
    if not answer:
        feedback = getattr(resp, "prompt_feedback", None)
        block_reason = getattr(feedback, "block_reason", None) if feedback else None
        # Log details for debugging (do not expose internals to end user)
        if resp_text_error:
            rag_logger.warning(
                "GenAI response parsing error: %s; prompt_block=%s",
                resp_text_error,
                bool(block_reason),
            )
        if block_reason:
            rag_logger.info("GenAI blocked request: %s", block_reason)
            answer = (
                "Xin lá»—i, yÃªu cáº§u cá»§a báº¡n bá»‹ mÃ´ hÃ¬nh tá»« chá»‘i do chÃ­nh sÃ¡ch ná»™i dung. "
                "Vui lÃ²ng thá»­ diá»…n Ä‘áº¡t láº¡i cÃ¢u há»i hoáº·c giáº£m bá»›t ná»™i dung nháº¡y cáº£m."
            )
        else:
            answer = (
                "Xin lá»—i, mÃ´ hÃ¬nh khÃ´ng tráº£ vá» ná»™i dung há»£p lá»‡. Vui lÃ²ng thá»­ láº¡i sau hoáº·c "
                "chá»‰nh sá»­a cÃ¢u há»i."
            )

    if passages:
        scores = [_sigmoid(p.score) for p in passages]
        conf = sum(scores) / len(scores)
    else:
        conf = 0.0
    return answer, float(conf)
