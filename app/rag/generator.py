from __future__ import annotations
import os
from typing import List, Tuple
import google.generativeai as genai
from app.utils.schema import Chunk

LLM_MODEL = os.getenv("RAG_LLM_MODEL", "gemini-1.5-flash")
TEMP = float(os.getenv("GEN_TEMPERATURE", "0.1"))
MAX_OUT = int(
    os.getenv("GEN_MAX_OUTPUT_TOKENS", "768")
)  # CÃ¢n báº±ng: 768 tokens Ä‘á»§ chi tiáº¿t nhÆ°ng tiáº¿t kiá»‡m hÆ¡n 1024

SYSTEM_PROMPT = (
    "Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn nghiá»‡p, tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u vá»›i format rÃµ rÃ ng vÃ  dá»… Ä‘á»c.\n\n"
    
    "ğŸ“‹ **Cáº¤U TRÃšC TRáº¢ Lá»œI Báº®T BUá»˜C:**\n\n"
    
    "**[CÃ¢u má»Ÿ Ä‘áº§u thÃ¢n thiá»‡n]**\n\n"
    
    "**ğŸ” ThÃ´ng tin chÃ­nh:**\n"
    "â€¢ Äiá»ƒm 1 [doc:page]\n"
    "â€¢ Äiá»ƒm 2 [doc:page]\n"
    "â€¢ Äiá»ƒm 3 [doc:page]\n\n"
    
    "**ï¿½ Chi tiáº¿t cá»¥ thá»ƒ:**\n"
    "[Giáº£i thÃ­ch chi tiáº¿t vá»›i vÃ­ dá»¥]\n\n"
    
    "**âš ï¸ LÆ°u Ã½ quan trá»ng:**\n"
    "â€¢ Äiá»u cáº§n chÃº Ã½ 1\n"
    "â€¢ Äiá»u cáº§n chÃº Ã½ 2\n\n"
    
    "**ğŸ’¡ TÃ³m táº¯t:**\n"
    "[Káº¿t luáº­n ngáº¯n gá»n]\n\n"
    
    "ğŸš¨ **QUY Táº®C QUAN TRá»ŒNG:**\n"
    "â€¢ LUÃ”N xuá»‘ng dÃ²ng giá»¯a cÃ¡c pháº§n\n"
    "â€¢ Sá»­ dá»¥ng ** ** Ä‘á»ƒ in Ä‘áº­m tiÃªu Ä‘á»\n"
    "â€¢ DÃ¹ng bullet points (â€¢) cho danh sÃ¡ch\n"
    "â€¢ LUÃ”N chÃ¨n [doc:page] sau thÃ´ng tin quan trá»ng\n"
    "â€¢ Viáº¿t Ä‘áº§y Ä‘á»§, khÃ´ng cáº¯t giá»¯a chá»«ng\n"
    "â€¢ Náº¿u khÃ´ng cÃ³ thÃ´ng tin: 'Xin lá»—i, khÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y trong tÃ i liá»‡u.'"
)


def _ensure_init():
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("GEMINI_API_KEY chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh.")
    genai.configure(api_key=api_key)


def _build_context(passages: List[Chunk]) -> str:
    parts = []
    for p in passages:
        header = f"[{p.doc_name}:{p.page}]"
        text = p.text.replace("\n", " ").strip()
        # CÃ¢n báº±ng: giáº£m tá»« 1500 xuá»‘ng 1200 Ä‘á»ƒ tiáº¿t kiá»‡m token nhÆ°ng váº«n Ä‘á»§ thÃ´ng tin
        if len(text) > 1200:
            text = text[:1200] + "â€¦"
        parts.append(f"{header} {text}")
    return "\n\n".join(parts)  # ThÃªm khoáº£ng cÃ¡ch giá»¯a cÃ¡c Ä‘oáº¡n


def generate(query: str, passages: List[Chunk]) -> Tuple[str, float]:
    _ensure_init()
    context = _build_context(passages)

    # Prompt tá»‘i Æ°u cho format Ä‘áº¹p vÃ  cÃ¢u tráº£ lá»i Ä‘áº§y Ä‘á»§
    prompt = f"""{SYSTEM_PROMPT}

=== THÃ”NG TIN Tá»ª TÃ€I LIá»†U ===
{context}

=== CÃ‚U Há»I ===
{query}

=== YÃŠU Cáº¦U ===
HÃ£y tráº£ lá»i THEO ÄÃšNG Cáº¤U TRÃšC trÃªn vá»›i:
- Xuá»‘ng dÃ²ng rÃµ rÃ ng giá»¯a cÃ¡c pháº§n
- Sá»­ dá»¥ng **bold** cho tiÃªu Ä‘á» 
- Bullet points cho danh sÃ¡ch
- Tráº£ lá»i Äáº¦Y Äá»¦, khÃ´ng Ä‘Æ°á»£c cáº¯t giá»¯a chá»«ng
- LuÃ´n cÃ³ trÃ­ch dáº«n [doc:page]"""

    model = genai.GenerativeModel(
        LLM_MODEL,
        generation_config={
            "temperature": 0.1,  # Giáº£m Ä‘á»ƒ Ä‘áº£m báº£o tuÃ¢n thá»§ format
            "max_output_tokens": MAX_OUT,
            "candidate_count": 1,
        },
    )
    resp = model.generate_content(prompt)
    answer = (resp.text or "").strip() if hasattr(resp, "text") else str(resp)

    if passages:
        scores = [float(p.score or 0.0) for p in passages]
        conf = sum((s + 1) / 2 for s in scores) / len(scores)
    else:
        conf = 0.0
    return answer, float(conf)
