from __future__ import annotations
import os
import math
from typing import List, Tuple
import google.generativeai as genai
from app.utils.schema import Chunk
from app.utils.logger import rag_logger

LLM_MODEL = os.getenv("RAG_LLM_MODEL", "gemini-2.0-flash-001")
TEMP = float(os.getenv("GEN_TEMPERATURE", "0.3"))
MAX_OUT = int(
    os.getenv("GEN_MAX_OUTPUT_TOKENS", "1024")
)  # TÄƒng Ä‘á»ƒ há»— trá»£ suy luáº­n chi tiáº¿t hÆ¡n

SYSTEM_PROMPT = (
    "Báº¡n lÃ  trá»£ lÃ½ RAG thÃ´ng minh, tráº£ lá»i báº±ng tiáº¿ng Viá»‡t dá»±a 100% trÃªn Ná»˜I DUNG TÃ€I LIá»†U. "
    "LuÃ´n suy luáº­n LOGIC vÃ  THÃ”NG MINH dá»±a trÃªn dáº«n chá»©ng tá»« tÃ i liá»‡u.\n"
    "ðŸ“‹ QUY TRÃŒNH SUY LUáº¬N:\n"
    "1. ðŸ” PhÃ¢n tÃ­ch cÃ¢u há»i: XÃ¡c Ä‘á»‹nh Ã½ chÃ­nh, loáº¡i cÃ¢u há»i (thá»±c táº¿, so sÃ¡nh, quy trÃ¬nh, Ä‘á»‹nh nghÄ©a).\n"
    "2. ðŸ•µï¸â€â™‚ï¸ TÃ¬m kiáº¿m dáº«n chá»©ng: So khá»›p thÃ´ng tin tá»« tÃ i liá»‡u vá»›i cÃ¢u há»i.\n"
    "3. ðŸ§  Suy luáº­n logic: Káº¿t ná»‘i thÃ´ng tin, so sÃ¡nh náº¿u cáº§n, loáº¡i bá» mÃ¢u thuáº«n.\n"
    "4. ðŸŽ¯ Káº¿t luáº­n: Tráº£ lá»i trá»±c tiáº¿p, rÃµ rÃ ng, CHI TIáº¾T vÃ  Cá»¤ THá»‚.\n"
    "ðŸ“Œ QUY Táº®C:\n"
    "â€¢ KhÃ´ng bá»‹a Ä‘áº·t; náº¿u thiáº¿u dá»¯ liá»‡u, tráº£ lá»i: 'Xin lá»—i, khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p trong tÃ i liá»‡u hiá»‡n cÃ³.'\n"
    "â€¢ Chá»‰ dÃ¹ng thÃ´ng tin tá»« tÃ i liá»‡u; khÃ´ng thÃªm kiáº¿n thá»©c ngoÃ i.\n"
    "â€¢ Khi trÃ­ch dáº«n, ghi Ä‘Ãºng tÃªn file vÃ  sá»‘ trang, vÃ­ dá»¥ [tb741.pdf:3].\n"
    "â€¢ Suy luáº­n tá»«ng bÆ°á»›c náº¿u cÃ¢u há»i phá»©c táº¡p, nhÆ°ng giá»¯ ngáº¯n gá»n.\n"
    "â€¢ TrÃ¬nh bÃ y báº±ng markdown rÃµ rÃ ng, chuyÃªn nghiá»‡p vÃ  thÃ¢n thiá»‡n.\n"
    "â€¢ **LUÃ”N TRáº¢ Lá»œI CHI TIáº¾T VÃ€ Cá»¤ THá»‚**: Cung cáº¥p Ä‘áº§y Ä‘á»§ thÃ´ng tin, vÃ­ dá»¥ thá»±c táº¿, sá»‘ liá»‡u chÃ­nh xÃ¡c, bÆ°á»›c thá»±c hiá»‡n rÃµ rÃ ng."
)


def _sigmoid(value: float | None) -> float:
    if value is None:
        return 0.0
    try:
        v = float(value)
    except (TypeError, ValueError):
        return 0.0
    if v >= 50:
        return 1.0
    if v <= -50:
        return 0.0
    return 1.0 / (1.0 + math.exp(-v))


def _ensure_init():
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("GEMINI_API_KEY chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh.")
    genai.configure(api_key=api_key)


def _build_context(passages: List[Chunk]) -> str:
    parts = []
    for idx, p in enumerate(passages, start=1):
        relevance = 0.0
        if isinstance(getattr(p, "meta", None), dict):
            relevance = float(p.meta.get("relevance", 0.0) or 0.0)
        elif getattr(p, "score", None) is not None:
            relevance = _sigmoid(p.score)

        # âœ… FIX: Æ¯u tiÃªn "filename" tá»« metadata, fallback vá» doc_name
        filename = p.doc_name  # Default
        if isinstance(getattr(p, "meta", None), dict):
            filename = p.meta.get("filename", p.meta.get("doc", p.doc_name))

        header = f"[{idx}] {filename}:{p.page} (Ä‘á»™ phÃ¹ há»£p â‰ˆ {relevance:.2f})"
        text = p.text.replace("\n", " ").strip()
        if len(text) > 1200:
            text = text[:1200] + "â€¦"
        parts.append(f"{header}\n{text}")
    return "\n\n".join(parts)


def _detect_question_type(query: str) -> str:
    """Detect the type of question for adaptive reasoning."""
    query_lower = query.lower()

    # Comparison questions
    if any(
        word in query_lower
        for word in ["so sÃ¡nh", "khÃ¡c nhau", "khÃ¡c biá»‡t", "so vá»›i", "tháº¿ nÃ o"]
    ):
        return "comparison"

    # Causal questions
    if any(
        word in query_lower
        for word in ["táº¡i sao", "vÃ¬ sao", "nguyÃªn nhÃ¢n", "do", "bá»Ÿi"]
    ):
        return "causal"

    # Procedural questions
    if any(
        word in query_lower
        for word in ["cÃ¡ch", "bÆ°á»›c", "quy trÃ¬nh", "lÃ m tháº¿ nÃ o", "thá»±c hiá»‡n"]
    ):
        return "procedural"

    # Definitional questions
    if any(
        word in query_lower
        for word in ["lÃ  gÃ¬", "Ä‘á»‹nh nghÄ©a", "giáº£i thÃ­ch", "cÃ³ nghÄ©a lÃ "]
    ):
        return "definitional"

    # Quantitative questions
    if any(
        word in query_lower for word in ["bao nhiÃªu", "máº¥y", "sá»‘ lÆ°á»£ng", "pháº§n trÄƒm"]
    ):
        return "quantitative"

    return "general"


def _get_reasoning_guide(question_type: str) -> str:
    """Get adaptive reasoning guide based on question type."""
    guides = {
        "comparison": """HÃ£y suy luáº­n Ä‘á»ƒ so sÃ¡nh CHI TIáº¾T:
1. **XÃ¡c Ä‘á»‹nh Ä‘á»‘i tÆ°á»£ng**: Liá»‡t kÃª cÃ¡c thá»±c thá»ƒ cáº§n so sÃ¡nh vá»›i mÃ´ táº£ Ä‘áº§y Ä‘á»§.
2. **TÃ¬m Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng**: TÃ¬m vÃ  giáº£i thÃ­ch cÃ¡c Ä‘áº·c Ä‘iá»ƒm chung má»™t cÃ¡ch cá»¥ thá»ƒ.
3. **TÃ¬m Ä‘iá»ƒm khÃ¡c biá»‡t**: XÃ¡c Ä‘á»‹nh vÃ  phÃ¢n tÃ­ch sá»± khÃ¡c nhau vá» Ä‘áº·c Ä‘iá»ƒm, Æ°u/nhÆ°á»£c Ä‘iá»ƒm chi tiáº¿t.
4. **Káº¿t luáº­n**: Tá»•ng há»£p so sÃ¡nh má»™t cÃ¡ch logic vá»›i vÃ­ dá»¥ cá»¥ thá»ƒ.""",
        "causal": """HÃ£y suy luáº­n vá» nguyÃªn nhÃ¢n CHI TIáº¾T:
1. **XÃ¡c Ä‘á»‹nh hiá»‡n tÆ°á»£ng**: MÃ´ táº£ váº¥n Ä‘á» hoáº·c káº¿t quáº£ vá»›i Ä‘áº§y Ä‘á»§ chi tiáº¿t.
2. **TÃ¬m nguyÃªn nhÃ¢n trá»±c tiáº¿p**: TÃ¬m vÃ  giáº£i thÃ­ch cÃ¡c yáº¿u tá»‘ gÃ¢y ra trá»±c tiáº¿p vá»›i vÃ­ dá»¥.
3. **TÃ¬m nguyÃªn nhÃ¢n giÃ¡n tiáº¿p**: Xem xÃ©t vÃ  phÃ¢n tÃ­ch cÃ¡c yáº¿u tá»‘ ná»n táº£ng chi tiáº¿t.
4. **Káº¿t luáº­n**: Giáº£i thÃ­ch má»‘i quan há»‡ nhÃ¢n quáº£ vá»›i báº±ng chá»©ng cá»¥ thá»ƒ.""",
        "procedural": """HÃ£y suy luáº­n vá» quy trÃ¬nh CHI TIáº¾T:
1. **XÃ¡c Ä‘á»‹nh má»¥c tiÃªu**: MÃ´ táº£ káº¿t quáº£ mong muá»‘n vá»›i Ä‘áº§y Ä‘á»§ chi tiáº¿t.
2. **Liá»‡t kÃª bÆ°á»›c thá»±c hiá»‡n**: TÃ¬m vÃ  mÃ´ táº£ cÃ¡c bÆ°á»›c theo thá»© tá»± logic vá»›i hÆ°á»›ng dáº«n cá»¥ thá»ƒ.
3. **XÃ¡c Ä‘á»‹nh Ä‘iá»u kiá»‡n**: LÆ°u Ã½ chi tiáº¿t cÃ¡c yÃªu cáº§u hoáº·c Ä‘iá»u kiá»‡n tiÃªn quyáº¿t.
4. **Káº¿t luáº­n**: TÃ³m táº¯t quy trÃ¬nh má»™t cÃ¡ch cÃ³ há»‡ thá»‘ng vá»›i vÃ­ dá»¥ thá»±c táº¿.""",
        "definitional": """HÃ£y suy luáº­n vá» Ä‘á»‹nh nghÄ©a CHI TIáº¾T:
1. **XÃ¡c Ä‘á»‹nh khÃ¡i niá»‡m**: MÃ´ táº£ Ä‘á»‘i tÆ°á»£ng cáº§n Ä‘á»‹nh nghÄ©a vá»›i ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§.
2. **TÃ¬m Ä‘áº·c Ä‘iá»ƒm chÃ­nh**: Liá»‡t kÃª vÃ  giáº£i thÃ­ch cÃ¡c thuá»™c tÃ­nh quan trá»ng chi tiáº¿t.
3. **TÃ¬m vÃ­ dá»¥**: TÃ¬m vÃ  mÃ´ táº£ minh há»a thá»±c táº¿ cá»¥ thá»ƒ.
4. **Káº¿t luáº­n**: ÄÆ°a ra Ä‘á»‹nh nghÄ©a rÃµ rÃ ng, Ä‘áº§y Ä‘á»§ vá»›i cÃ¡c khÃ­a cáº¡nh khÃ¡c nhau.""",
        "quantitative": """HÃ£y suy luáº­n vá» sá»‘ lÆ°á»£ng CHI TIáº¾T:
1. **XÃ¡c Ä‘á»‹nh chá»‰ sá»‘**: MÃ´ táº£ dá»¯ liá»‡u cáº§n tÃ¬m vá»›i ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§.
2. **TÃ¬m sá»‘ liá»‡u cá»¥ thá»ƒ**: TrÃ­ch xuáº¥t vÃ  phÃ¢n tÃ­ch cÃ¡c con sá»‘ tá»« tÃ i liá»‡u chi tiáº¿t.
3. **XÃ¡c minh tÃ­nh chÃ­nh xÃ¡c**: Kiá»ƒm tra nguá»“n vÃ  ngá»¯ cáº£nh vá»›i giáº£i thÃ­ch cá»¥ thá»ƒ.
4. **Káº¿t luáº­n**: TrÃ¬nh bÃ y sá»‘ liá»‡u vá»›i Ä‘Æ¡n vá»‹, ngá»¯ cáº£nh vÃ  phÃ¢n tÃ­ch chi tiáº¿t.""",
        "general": """HÃ£y suy luáº­n tá»«ng bÆ°á»›c Ä‘á»ƒ tráº£ lá»i CHI TIáº¾T vÃ  THÃ”NG MINH:
1. **PhÃ¢n tÃ­ch cÃ¢u há»i**: XÃ¡c Ä‘á»‹nh loáº¡i cÃ¢u há»i vÃ  thÃ´ng tin cáº§n thiáº¿t vá»›i phÃ¢n tÃ­ch sÃ¢u.
2. **TÃ¬m kiáº¿m dáº«n chá»©ng**: TrÃ­ch xuáº¥t vÃ  phÃ¢n tÃ­ch thÃ´ng tin liÃªn quan tá»« tÃ i liá»‡u chi tiáº¿t.
3. **Suy luáº­n logic**: Káº¿t ná»‘i thÃ´ng tin, so sÃ¡nh náº¿u cÃ³ nhiá»u nguá»“n, loáº¡i bá» thÃ´ng tin khÃ´ng liÃªn quan vá»›i giáº£i thÃ­ch.
4. **Káº¿t luáº­n**: Tráº£ lá»i trá»±c tiáº¿p, rÃµ rÃ ng vá»›i Ä‘áº§y Ä‘á»§ chi tiáº¿t vÃ  báº±ng chá»©ng.""",
    }

    return guides.get(question_type, guides["general"])


def generate(query: str, passages: List[Chunk]) -> Tuple[str, float]:
    _ensure_init()
    context = _build_context(passages)

    # Adjust max output for summary queries
    is_summary = any(
        word in query.lower()
        for word in ["tÃ³m táº¯t", "tá»•ng káº¿t", "summary", "summarize", "tÃ³m táº¯t ná»™i dung"]
    )
    max_out = MAX_OUT * 2 if is_summary else MAX_OUT  # Double for summaries

    question_type = _detect_question_type(query)
    reasoning_guide = _get_reasoning_guide(question_type)

    # Enhanced prompt: yÃªu cáº§u khÃ´ng láº·p láº¡i citation giá»‘ng nhau liÃªn tá»¥c
    citation_note = (
        "\n\n**LÆ°u Ã½ vá» trÃ­ch dáº«n:**\n"
        "- Chá»‰ chÃ¨n trÃ­ch dáº«n [tÃªn_file.pdf:trang] á»Ÿ cuá»‘i Ä‘oáº¡n hoáº·c bullet lá»›n nháº¥t, khÃ´ng láº·p láº¡i cÃ¹ng má»™t trÃ­ch dáº«n nhiá»u láº§n liÃªn tiáº¿p trong cÃ¹ng má»™t Ä‘oáº¡n hoáº·c danh sÃ¡ch.\n"
        "- Náº¿u nhiá»u Ã½ trong cÃ¹ng Ä‘oáº¡n/bullet cÃ¹ng nguá»“n, chá»‰ cáº§n citation á»Ÿ cuá»‘i Ä‘oáº¡n/bullet Ä‘Ã³.\n"
        "- KhÃ´ng chÃ¨n citation sau tá»«ng cÃ¢u nhá» náº¿u cÃ¹ng nguá»“n.\n"
    )
    if is_summary:
        prompt = f"""{SYSTEM_PROMPT}

=== THÃ”NG TIN Tá»ª TÃ€I LIá»†U ===
{context}

=== CÃ‚U Há»ŽI ===
{query}

=== HÆ¯á»šNG DáºªN SUY LUáº¬N ===
HÃ£y suy luáº­n thÃ´ng minh Ä‘á»ƒ tÃ³m táº¯t:
1. **PhÃ¢n tÃ­ch tá»•ng thá»ƒ**: XÃ¡c Ä‘á»‹nh chá»§ Ä‘á» chÃ­nh vÃ  má»‘i liÃªn há»‡ giá»¯a cÃ¡c tÃ i liá»‡u.
2. **TrÃ­ch xuáº¥t thÃ´ng tin**: Láº¥y Ä‘iá»ƒm chÃ­nh, sá»‘ liá»‡u, quy trÃ¬nh tá»« má»—i nguá»“n.
3. **So sÃ¡nh vÃ  tá»•ng há»£p**: Äá»‘i chiáº¿u thÃ´ng tin, loáº¡i bá» trÃ¹ng láº·p, táº¡o báº£n Ä‘á»“ logic.
4. **Káº¿t luáº­n**: TÃ³m táº¯t cÃ´ Ä‘á»ng nhÆ°ng Ä‘áº§y Ä‘á»§.

=== YÃŠU Cáº¦U TRÃŒNH BÃ€Y ===
- **TÃ³m táº¯t tá»•ng quÃ¡t**: Ná»™i dung chÃ­nh cá»§a tá»«ng file vá»›i CHI TIáº¾T Ä‘áº§y Ä‘á»§.
- **Äiá»ƒm quan trá»ng**: Liá»‡t kÃª cÃ¡c Ä‘iá»ƒm chÃ­nh, sá»‘ liá»‡u, quy trÃ¬nh Cá»¤ THá»‚ vÃ  chi tiáº¿t.
- **So sÃ¡nh náº¿u cÃ³**: Äá»‘i chiáº¿u ná»™i dung liÃªn quan giá»¯a cÃ¡c file má»™t cÃ¡ch CHI TIáº¾T.
- **Káº¿t luáº­n**: TÃ³m táº¯t chung vá»›i logic rÃµ rÃ ng vÃ  Ä‘áº§y Ä‘á»§ thÃ´ng tin.
Sá»­ dá»¥ng markdown, trÃ­ch dáº«n chÃ­nh xÃ¡c [tÃªn_file.pdf:trang], Ä‘áº£m báº£o logic vÃ  Ä‘áº§y Ä‘á»§.
{citation_note}"""
    else:
        prompt = f"""{SYSTEM_PROMPT}

=== THÃ”NG TIN Tá»ª TÃ€I LIá»†U ===
{context}

=== CÃ‚U Há»ŽI ===
{query}

=== HÆ¯á»šNG DáºªN SUY LUáº¬N ===
{reasoning_guide}

=== YÃŠU Cáº¦U TRÃŒNH BÃ€Y ===
- **Káº¿t luáº­n**: CÃ¢u tráº£ lá»i trá»±c tiáº¿p, CHI TIáº¾T vÃ  Cá»¤ THá»‚ (2-5 cÃ¢u).
- **Dáº«n chá»©ng**: 2-4 Ä‘iá»ƒm chÃ­nh tá»« tÃ i liá»‡u vá»›i giáº£i thÃ­ch Ä‘áº§y Ä‘á»§, kÃ¨m [tÃªn_file.pdf:trang].
- **PhÃ¢n tÃ­ch** (náº¿u cáº§n): Giáº£i thÃ­ch logic chi tiáº¿t, so sÃ¡nh náº¿u cÃ³ nhiá»u thÃ´ng tin.
- **Khuyáº¿n nghá»‹** (náº¿u phÃ¹ há»£p): HÃ nh Ä‘á»™ng tiáº¿p theo hoáº·c lÆ°u Ã½ Cá»¤ THá»‚.
Sá»­ dá»¥ng markdown, trÃ­ch dáº«n chÃ­nh xÃ¡c, Ä‘áº£m báº£o logic vÃ  Ä‘áº§y Ä‘á»§.
{citation_note}"""

    model = genai.GenerativeModel(
        LLM_MODEL,
        generation_config={
            "temperature": TEMP,
            "max_output_tokens": max_out,
            "candidate_count": 1,
        },
    )
    resp = model.generate_content(prompt)

    feedback = getattr(resp, "prompt_feedback", None)
    if feedback and getattr(feedback, "block_reason", None):
        raise RuntimeError(f"YÃªu cáº§u bá»‹ tá»« chá»‘i: {feedback.block_reason}")

    answer = ""
    resp_text_error: str | None = None
    # resp.text accessor may raise ValueError if the response doesn't contain Part objects
    try:
        if hasattr(resp, "text") and resp.text:
            answer = resp.text.strip()
    except ValueError as e:
        # keep the error message for debugging / fallback behavior
        resp_text_error = str(e)

    # Fallback: try to extract from candidates (older or alternate response shape)
    if not answer and getattr(resp, "candidates", None):
        for candidate in resp.candidates:
            parts = getattr(getattr(candidate, "content", None), "parts", [])
            texts = [p.text for p in parts if getattr(p, "text", None)]
            if texts:
                answer = "\n".join(texts).strip()
                if answer:
                    break

    # If still empty, handle gracefully (do not raise unhandled exception)
    if not answer:
        feedback = getattr(resp, "prompt_feedback", None)
        block_reason = getattr(feedback, "block_reason", None) if feedback else None
        # Log details for debugging (do not expose internals to end user)
        if resp_text_error:
            rag_logger.warning(
                "GenAI response parsing error: %s; prompt_block=%s",
                resp_text_error,
                bool(block_reason),
            )
        if block_reason:
            rag_logger.info("GenAI blocked request: %s", block_reason)
            answer = (
                "Xin lá»—i, yÃªu cáº§u cá»§a báº¡n bá»‹ mÃ´ hÃ¬nh tá»« chá»‘i do chÃ­nh sÃ¡ch ná»™i dung. "
                "Vui lÃ²ng thá»­ diá»…n Ä‘áº¡t láº¡i cÃ¢u há»i hoáº·c giáº£m bá»›t ná»™i dung nháº¡y cáº£m."
            )
        else:
            answer = (
                "Xin lá»—i, mÃ´ hÃ¬nh khÃ´ng tráº£ vá» ná»™i dung há»£p lá»‡. Vui lÃ²ng thá»­ láº¡i sau hoáº·c "
                "chá»‰nh sá»­a cÃ¢u há»i."
            )

    conf = 0.0
    if passages:
        chunk_scores: list[float] = []
        for p in passages:
            meta = getattr(p, "meta", {}) or {}
            candidates: list[float] = []
            for key in ("relevance", "hybrid_score", "dense_score", "bm25_score"):
                val = meta.get(key)
                if val is None:
                    continue
                try:
                    candidates.append(float(val))
                except (TypeError, ValueError):
                    continue
            if not candidates and getattr(p, "score", None) is not None:
                candidates.append(_sigmoid(p.score))
            if not candidates:
                continue
            best = max(candidates)
            chunk_scores.append(max(0.0, min(1.0, best)))

        if chunk_scores:
            chunk_scores.sort(reverse=True)
            primary = chunk_scores[0]
            support_vals = chunk_scores[1:3]
            support = sum(support_vals) / len(support_vals) if support_vals else primary
            high_quality = len([s for s in chunk_scores if s >= 0.6])
            coverage_ratio = min(1.0, high_quality / max(1, len(chunk_scores)))
            consistency = 1.0 - min(1.0, abs(primary - support))

            blended = (
                0.6 * primary
                + 0.25 * support
                + 0.1 * coverage_ratio
                + 0.05 * consistency
            )

            if primary >= 0.85:
                blended = max(blended, 0.92)
            elif primary >= 0.75:
                blended = max(blended, 0.84)
            elif primary >= 0.65:
                blended = max(blended, 0.76)

            conf = min(0.99, max(primary * 0.9, blended))

    return answer, float(conf)
